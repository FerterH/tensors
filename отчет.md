Отчет. Основы PyTorch

Я прочитал homework.md, чтобы понять список заданий и что нужно сделать.

Я создал файл homework_tensors.py и сделал тензоры нужных размеров со случайными числами, нулями, единицами и последовательностью от 0 до 15.

Я проверил операции с тензорами: транспонирование, матричное умножение, поэлементное умножение и сумму элементов.

Я потренировался с индексами: взял строки, столбцы, центральную подматрицу 2x2 и элементы с четными индексами из тензора 5x5x5.

Я менял форму тензора из 24 элементов в разные размеры, чтобы убедиться, что reshape работает как нужно.

Я создал файл homework_autograd.py, задал тензоры с requires_grad и посчитал f(x, y, z) = x^2 + y^2 + z^2 + 2*x*y*z, потом посмотрел градиенты.

Я реализовал функцию MSE для y_pred = w*x + b, вывел градиенты по w и b и сверил с формулами.

Я проверил цепное правило на функции f(x) = sin(x^2 + 1) и сравнил результат с torch.autograd.grad.

Я создал файл homework_performance.py, сделал большие матрицы разных размеров на случайных данных.

Я написал функцию измерения времени на CPU и GPU с torch.cuda.Event и time.time

Я сравнил операции матричного умножения, поэлементного сложения, умножения, транспонирования и суммы на CPU и GPU, посчитал ускорение. Результаты следущие:

Операция          | CPU (мс) | GPU (мс) | Ускорение
Матричное умножение | 55.3  |   82.4   |   0.7
Поэлементное сложение | 11.3  |   19.3   |   0.6
Поэлементное умножение | 11.4  |   9.6   |   1.2
Транспонирование | 0.1  |   0.1   |   1.6
Сумма элементов | 1.3  |   12.3   |   0.1

Я сделал короткий вывод, какие операции ускоряются сильнее, как влияет размер матрицы и что передача данных между CPU и GPU может замедлять расчет.

Я добавил простые проверки размеров, docstring и убедился, что код работает и без GPU.


